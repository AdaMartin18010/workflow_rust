# ÊåÅ‰πÖÂåñÂÆûÁé∞

## üìã ÊñáÊ°£Ê¶ÇËø∞

Êú¨ÊñáÊ°£ËØ¶ÁªÜÈòêËø∞TemporalÁöÑÊåÅ‰πÖÂåñÂ±ÇÂÆûÁé∞ÔºåÂåÖÊã¨Ôºö

- ÊåÅ‰πÖÂåñÊû∂ÊûÑ
- ‰∫ã‰ª∂Â≠òÂÇ®
- Rust 1.90ÂÆûÁé∞
- GolangÂÆûÁé∞ÂØπÊØî
- Â§öÁßçÂ≠òÂÇ®ÂêéÁ´Ø
- ÊÄßËÉΩ‰ºòÂåñ

---

## üéØ ÊåÅ‰πÖÂåñÊ†∏ÂøÉÊ¶ÇÂøµ

### ‰∏∫‰ªÄ‰πàÈúÄË¶ÅÊåÅ‰πÖÂåñÔºü

TemporalÁöÑÊ†∏ÂøÉËÉΩÂäõ‰æùËµñ‰∫éÊåÅ‰πÖÂåñÔºö

1. **ÊåÅ‰πÖÊÄß (Durability)**: Â∑•‰ΩúÊµÅÁä∂ÊÄÅÂú®ËøõÁ®ãÂ¥©Ê∫ÉÂêéÂèØÊÅ¢Â§ç
2. **‰∫ã‰ª∂Ê∫ØÊ∫ê (Event Sourcing)**: ÈÄöËøá‰∫ã‰ª∂ÂéÜÂè≤ÈáçÂª∫Áä∂ÊÄÅ
3. **Á°ÆÂÆöÊÄßÈáçÊîæ (Deterministic Replay)**: ÊîØÊåÅ‰ª£Á†ÅÊõ¥Êñ∞
4. **ÂèØËßÇÊµãÊÄß**: Êü•ËØ¢ÂéÜÂè≤ÊâßË°åËÆ∞ÂΩï

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ÊåÅ‰πÖÂåñÊû∂ÊûÑ                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Worker                  Temporal Service            Storage
  ‚îÇ                           ‚îÇ                        ‚îÇ
  ‚îú‚îÄ Execute Workflow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ                        ‚îÇ
  ‚îÇ                           ‚îú‚îÄ Save Event ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ
  ‚îÇ                           ‚îÇ                        ‚îÇ
  ‚îÇ  ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ Crash ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ       ‚îÇ                        ‚îÇ
  ‚îÇ                           ‚îÇ                        ‚îÇ
  ‚îú‚îÄ Restart ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ                        ‚îÇ
  ‚îÇ                           ‚îú‚îÄ Load Events ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ                           ‚îÇ                        ‚îÇ
  ‚îú‚îÄ Replay Events ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                        ‚îÇ
  ‚îÇ                           ‚îÇ                        ‚îÇ
  ‚îî‚îÄ Continue Execution ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ                        ‚îÇ
```

---

## ü¶Ä RustÂÆûÁé∞

### Â≠òÂÇ®Êé•Âè£ÂÆö‰πâ

```rust
use async_trait::async_trait;
use serde::{Serialize, Deserialize};

/// Â∑•‰ΩúÊµÅÂ≠òÂÇ®trait
#[async_trait]
pub trait WorkflowStorage: Send + Sync {
    /// ‰øùÂ≠òÂ∑•‰ΩúÊµÅÊâßË°å
    async fn save_workflow_execution(
        &self,
        execution: &WorkflowExecution,
        metadata: &WorkflowMetadata,
    ) -> Result<(), StorageError>;
    
    /// Âä†ËΩΩÂ∑•‰ΩúÊµÅÊâßË°å
    async fn load_workflow_execution(
        &self,
        workflow_id: &WorkflowId,
    ) -> Result<Option<WorkflowExecutionRecord>, StorageError>;
    
    /// ËøΩÂä†‰∫ã‰ª∂
    async fn append_event(
        &self,
        workflow_id: &WorkflowId,
        event: WorkflowEvent,
    ) -> Result<EventId, StorageError>;
    
    /// Ëé∑Âèñ‰∫ã‰ª∂ÂéÜÂè≤
    async fn get_event_history(
        &self,
        workflow_id: &WorkflowId,
        from_event_id: Option<EventId>,
        limit: Option<usize>,
    ) -> Result<Vec<WorkflowEvent>, StorageError>;
    
    /// Êõ¥Êñ∞Â∑•‰ΩúÊµÅÁä∂ÊÄÅ
    async fn update_workflow_state(
        &self,
        workflow_id: &WorkflowId,
        state: WorkflowLifecycleState,
    ) -> Result<(), StorageError>;
    
    /// ‰øùÂ≠òActivityÂøÉË∑≥
    async fn save_activity_heartbeat(
        &self,
        activity_id: &ActivityId,
        details: serde_json::Value,
        timestamp: DateTime<Utc>,
    ) -> Result<(), StorageError>;
    
    /// Êü•ËØ¢Â∑•‰ΩúÊµÅ
    async fn query_workflows(
        &self,
        query: WorkflowQuery,
    ) -> Result<Vec<WorkflowExecutionRecord>, StorageError>;
}

/// Â∑•‰ΩúÊµÅÂÖÉÊï∞ÊçÆ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WorkflowMetadata {
    pub workflow_type: String,
    pub task_queue: String,
    pub started_at: DateTime<Utc>,
    pub timeout_config: TimeoutConfig,
    pub tags: HashMap<String, String>,
}

/// Â∑•‰ΩúÊµÅÊâßË°åËÆ∞ÂΩï
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WorkflowExecutionRecord {
    pub execution: WorkflowExecution,
    pub metadata: WorkflowMetadata,
    pub state: WorkflowLifecycleState,
    pub event_history: Vec<WorkflowEvent>,
    pub updated_at: DateTime<Utc>,
}

/// Â∑•‰ΩúÊµÅÊü•ËØ¢
#[derive(Debug, Clone)]
pub struct WorkflowQuery {
    pub workflow_type: Option<String>,
    pub state: Option<WorkflowLifecycleState>,
    pub started_after: Option<DateTime<Utc>>,
    pub started_before: Option<DateTime<Utc>>,
    pub limit: Option<usize>,
    pub offset: Option<usize>,
}
```

### PostgreSQLÂÆûÁé∞

```rust
use sqlx::{PgPool, Row};
use sqlx::postgres::PgRow;

/// PostgreSQLÂ≠òÂÇ®ÂÆûÁé∞
pub struct PostgresWorkflowStorage {
    pool: PgPool,
}

impl PostgresWorkflowStorage {
    /// ÂàõÂª∫Êñ∞ÂÆû‰æã
    pub async fn new(database_url: &str) -> Result<Self, StorageError> {
        let pool = PgPool::connect(database_url)
            .await
            .map_err(|e| StorageError::ConnectionError(e.to_string()))?;
        
        Ok(Self { pool })
    }
    
    /// ÂàùÂßãÂåñÊï∞ÊçÆÂ∫ìschema
    pub async fn init_schema(&self) -> Result<(), StorageError> {
        sqlx::query(
            r#"
            CREATE TABLE IF NOT EXISTS workflow_executions (
                workflow_id TEXT PRIMARY KEY,
                run_id TEXT NOT NULL,
                workflow_type TEXT NOT NULL,
                task_queue TEXT NOT NULL,
                state TEXT NOT NULL,
                started_at TIMESTAMPTZ NOT NULL,
                updated_at TIMESTAMPTZ NOT NULL,
                completed_at TIMESTAMPTZ,
                metadata JSONB NOT NULL,
                UNIQUE(workflow_id, run_id)
            );
            
            CREATE TABLE IF NOT EXISTS workflow_events (
                id BIGSERIAL PRIMARY KEY,
                workflow_id TEXT NOT NULL,
                event_id BIGINT NOT NULL,
                event_type TEXT NOT NULL,
                event_data JSONB NOT NULL,
                timestamp TIMESTAMPTZ NOT NULL,
                UNIQUE(workflow_id, event_id)
            );
            
            CREATE INDEX IF NOT EXISTS idx_workflow_events_workflow_id 
                ON workflow_events(workflow_id, event_id);
            
            CREATE TABLE IF NOT EXISTS activity_heartbeats (
                activity_id TEXT PRIMARY KEY,
                workflow_id TEXT NOT NULL,
                details JSONB,
                timestamp TIMESTAMPTZ NOT NULL
            );
            
            CREATE INDEX IF NOT EXISTS idx_activity_heartbeats_workflow_id 
                ON activity_heartbeats(workflow_id);
            "#
        )
        .execute(&self.pool)
        .await
        .map_err(|e| StorageError::QueryError(e.to_string()))?;
        
        Ok(())
    }
}

#[async_trait]
impl WorkflowStorage for PostgresWorkflowStorage {
    async fn save_workflow_execution(
        &self,
        execution: &WorkflowExecution,
        metadata: &WorkflowMetadata,
    ) -> Result<(), StorageError> {
        sqlx::query(
            r#"
            INSERT INTO workflow_executions (
                workflow_id, run_id, workflow_type, task_queue, 
                state, started_at, updated_at, metadata
            )
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
            ON CONFLICT (workflow_id) 
            DO UPDATE SET 
                run_id = EXCLUDED.run_id,
                updated_at = EXCLUDED.updated_at,
                metadata = EXCLUDED.metadata
            "#
        )
        .bind(&execution.workflow_id.as_str())
        .bind(&execution.run_id.to_string())
        .bind(&metadata.workflow_type)
        .bind(&metadata.task_queue)
        .bind("Running")
        .bind(metadata.started_at)
        .bind(Utc::now())
        .bind(serde_json::to_value(metadata).unwrap())
        .execute(&self.pool)
        .await
        .map_err(|e| StorageError::QueryError(e.to_string()))?;
        
        Ok(())
    }
    
    async fn load_workflow_execution(
        &self,
        workflow_id: &WorkflowId,
    ) -> Result<Option<WorkflowExecutionRecord>, StorageError> {
        let row = sqlx::query(
            r#"
            SELECT workflow_id, run_id, workflow_type, task_queue, 
                   state, started_at, updated_at, metadata
            FROM workflow_executions
            WHERE workflow_id = $1
            "#
        )
        .bind(workflow_id.as_str())
        .fetch_optional(&self.pool)
        .await
        .map_err(|e| StorageError::QueryError(e.to_string()))?;
        
        if let Some(row) = row {
            let execution = WorkflowExecution::with_run_id(
                WorkflowId::new(row.get::<String, _>("workflow_id")),
                RunId::parse(&row.get::<String, _>("run_id")).unwrap(),
            );
            
            let metadata: WorkflowMetadata = serde_json::from_value(
                row.get::<serde_json::Value, _>("metadata")
            ).unwrap();
            
            // Âä†ËΩΩ‰∫ã‰ª∂ÂéÜÂè≤
            let event_history = self.get_event_history(workflow_id, None, None).await?;
            
            Ok(Some(WorkflowExecutionRecord {
                execution,
                metadata,
                state: parse_state(row.get("state")),
                event_history,
                updated_at: row.get("updated_at"),
            }))
        } else {
            Ok(None)
        }
    }
    
    async fn append_event(
        &self,
        workflow_id: &WorkflowId,
        event: WorkflowEvent,
    ) -> Result<EventId, StorageError> {
        let row = sqlx::query(
            r#"
            INSERT INTO workflow_events (
                workflow_id, event_id, event_type, event_data, timestamp
            )
            VALUES ($1, $2, $3, $4, $5)
            RETURNING id
            "#
        )
        .bind(workflow_id.as_str())
        .bind(event.event_id.0 as i64)
        .bind(format!("{:?}", event.event_type))
        .bind(serde_json::to_value(&event.event_type).unwrap())
        .bind(event.timestamp)
        .fetch_one(&self.pool)
        .await
        .map_err(|e| StorageError::QueryError(e.to_string()))?;
        
        Ok(EventId(row.get::<i64, _>("id") as u64))
    }
    
    async fn get_event_history(
        &self,
        workflow_id: &WorkflowId,
        from_event_id: Option<EventId>,
        limit: Option<usize>,
    ) -> Result<Vec<WorkflowEvent>, StorageError> {
        let mut query = String::from(
            "SELECT event_id, event_type, event_data, timestamp FROM workflow_events WHERE workflow_id = $1"
        );
        
        if from_event_id.is_some() {
            query.push_str(" AND event_id >= $2");
        }
        
        query.push_str(" ORDER BY event_id ASC");
        
        if let Some(limit) = limit {
            query.push_str(&format!(" LIMIT {}", limit));
        }
        
        let mut sql_query = sqlx::query(&query).bind(workflow_id.as_str());
        
        if let Some(from_id) = from_event_id {
            sql_query = sql_query.bind(from_id.0 as i64);
        }
        
        let rows = sql_query
            .fetch_all(&self.pool)
            .await
            .map_err(|e| StorageError::QueryError(e.to_string()))?;
        
        let events = rows
            .into_iter()
            .map(|row| {
                WorkflowEvent {
                    event_id: EventId(row.get::<i64, _>("event_id") as u64),
                    timestamp: row.get("timestamp"),
                    event_type: serde_json::from_value(
                        row.get::<serde_json::Value, _>("event_data")
                    ).unwrap(),
                }
            })
            .collect();
        
        Ok(events)
    }
    
    async fn update_workflow_state(
        &self,
        workflow_id: &WorkflowId,
        state: WorkflowLifecycleState,
    ) -> Result<(), StorageError> {
        sqlx::query(
            r#"
            UPDATE workflow_executions 
            SET state = $1, updated_at = $2
            WHERE workflow_id = $3
            "#
        )
        .bind(format!("{:?}", state))
        .bind(Utc::now())
        .bind(workflow_id.as_str())
        .execute(&self.pool)
        .await
        .map_err(|e| StorageError::QueryError(e.to_string()))?;
        
        Ok(())
    }
    
    async fn save_activity_heartbeat(
        &self,
        activity_id: &ActivityId,
        details: serde_json::Value,
        timestamp: DateTime<Utc>,
    ) -> Result<(), StorageError> {
        sqlx::query(
            r#"
            INSERT INTO activity_heartbeats (activity_id, workflow_id, details, timestamp)
            VALUES ($1, $2, $3, $4)
            ON CONFLICT (activity_id)
            DO UPDATE SET details = EXCLUDED.details, timestamp = EXCLUDED.timestamp
            "#
        )
        .bind(activity_id.as_str())
        .bind("unknown")  // ÂÆûÈôÖÂ∫îËØ•‰ªéactivity_idÊèêÂèñ
        .bind(details)
        .bind(timestamp)
        .execute(&self.pool)
        .await
        .map_err(|e| StorageError::QueryError(e.to_string()))?;
        
        Ok(())
    }
    
    async fn query_workflows(
        &self,
        query: WorkflowQuery,
    ) -> Result<Vec<WorkflowExecutionRecord>, StorageError> {
        // ÊûÑÂª∫Âä®ÊÄÅÊü•ËØ¢
        let mut sql = String::from("SELECT * FROM workflow_executions WHERE 1=1");
        let mut bindings: Vec<String> = vec![];
        
        if let Some(workflow_type) = &query.workflow_type {
            sql.push_str(&format!(" AND workflow_type = ${}", bindings.len() + 1));
            bindings.push(workflow_type.clone());
        }
        
        if let Some(state) = &query.state {
            sql.push_str(&format!(" AND state = ${}", bindings.len() + 1));
            bindings.push(format!("{:?}", state));
        }
        
        sql.push_str(" ORDER BY started_at DESC");
        
        if let Some(limit) = query.limit {
            sql.push_str(&format!(" LIMIT {}", limit));
        }
        
        if let Some(offset) = query.offset {
            sql.push_str(&format!(" OFFSET {}", offset));
        }
        
        // ÊâßË°åÊü•ËØ¢Âπ∂ËΩ¨Êç¢ÁªìÊûú
        // (ÁÆÄÂåñÂÆûÁé∞)
        Ok(vec![])
    }
}
```

### ÂÜÖÂ≠òÂ≠òÂÇ®ÂÆûÁé∞ÔºàÊµãËØïÁî®Ôºâ

```rust
use std::collections::HashMap;
use tokio::sync::RwLock;

/// ÂÜÖÂ≠òÂ≠òÂÇ®ÂÆûÁé∞ÔºàÁî®‰∫éÊµãËØïÔºâ
pub struct InMemoryWorkflowStorage {
    executions: Arc<RwLock<HashMap<WorkflowId, WorkflowExecutionRecord>>>,
    events: Arc<RwLock<HashMap<WorkflowId, Vec<WorkflowEvent>>>>,
}

impl InMemoryWorkflowStorage {
    pub fn new() -> Self {
        Self {
            executions: Arc::new(RwLock::new(HashMap::new())),
            events: Arc::new(RwLock::new(HashMap::new())),
        }
    }
}

#[async_trait]
impl WorkflowStorage for InMemoryWorkflowStorage {
    async fn save_workflow_execution(
        &self,
        execution: &WorkflowExecution,
        metadata: &WorkflowMetadata,
    ) -> Result<(), StorageError> {
        let record = WorkflowExecutionRecord {
            execution: execution.clone(),
            metadata: metadata.clone(),
            state: WorkflowLifecycleState::Running,
            event_history: vec![],
            updated_at: Utc::now(),
        };
        
        self.executions
            .write()
            .await
            .insert(execution.workflow_id.clone(), record);
        
        Ok(())
    }
    
    async fn load_workflow_execution(
        &self,
        workflow_id: &WorkflowId,
    ) -> Result<Option<WorkflowExecutionRecord>, StorageError> {
        let executions = self.executions.read().await;
        Ok(executions.get(workflow_id).cloned())
    }
    
    async fn append_event(
        &self,
        workflow_id: &WorkflowId,
        event: WorkflowEvent,
    ) -> Result<EventId, StorageError> {
        let mut events = self.events.write().await;
        let workflow_events = events.entry(workflow_id.clone()).or_insert_with(Vec::new);
        workflow_events.push(event.clone());
        Ok(event.event_id)
    }
    
    async fn get_event_history(
        &self,
        workflow_id: &WorkflowId,
        from_event_id: Option<EventId>,
        limit: Option<usize>,
    ) -> Result<Vec<WorkflowEvent>, StorageError> {
        let events = self.events.read().await;
        
        if let Some(workflow_events) = events.get(workflow_id) {
            let mut filtered: Vec<_> = workflow_events.iter()
                .filter(|e| {
                    if let Some(from_id) = from_event_id {
                        e.event_id >= from_id
                    } else {
                        true
                    }
                })
                .cloned()
                .collect();
            
            if let Some(limit) = limit {
                filtered.truncate(limit);
            }
            
            Ok(filtered)
        } else {
            Ok(vec![])
        }
    }
    
    async fn update_workflow_state(
        &self,
        workflow_id: &WorkflowId,
        state: WorkflowLifecycleState,
    ) -> Result<(), StorageError> {
        let mut executions = self.executions.write().await;
        
        if let Some(record) = executions.get_mut(workflow_id) {
            record.state = state;
            record.updated_at = Utc::now();
        }
        
        Ok(())
    }
    
    // ÂÖ∂‰ªñÊñπÊ≥ïÂÆûÁé∞...
    async fn save_activity_heartbeat(
        &self,
        _activity_id: &ActivityId,
        _details: serde_json::Value,
        _timestamp: DateTime<Utc>,
    ) -> Result<(), StorageError> {
        Ok(())
    }
    
    async fn query_workflows(
        &self,
        _query: WorkflowQuery,
    ) -> Result<Vec<WorkflowExecutionRecord>, StorageError> {
        let executions = self.executions.read().await;
        Ok(executions.values().cloned().collect())
    }
}
```

---

## üêπ GolangÂÆûÁé∞ÂØπÊØî

### Temporal Go SDKÊåÅ‰πÖÂåñ

GolangÁöÑTemporal SDK‰ΩøÁî®Temporal ServiceÁöÑÊåÅ‰πÖÂåñÂ±ÇÔºå‰∏çÈúÄË¶ÅÁõ¥Êé•ÂÆûÁé∞Â≠òÂÇ®Êé•Âè£„ÄÇ

```go
// Temporal ServiceÈÖçÁΩÆÔºàÁÆ°ÁêÜÂëòÊìç‰ΩúÔºâ
import (
    "go.temporal.io/server/common/config"
)

// PostgreSQLÈÖçÁΩÆ
cfg := &config.Persistence{
    DefaultStore: "default",
    DataStores: map[string]config.DataStore{
        "default": {
            SQL: &config.SQL{
                PluginName:   "postgres",
                DatabaseName: "temporal",
                ConnectAddr:  "localhost:5432",
                User:         "temporal",
                Password:     "temporal",
            },
        },
    },
}
```

---

## üéØ ÊúÄ‰Ω≥ÂÆûË∑µ

### 1. ËøûÊé•Ê±†ÁÆ°ÁêÜ

```rust
// ‚úÖ Â•Ω: ‰ΩøÁî®ËøûÊé•Ê±†
let pool = PgPoolOptions::new()
    .max_connections(20)
    .min_connections(5)
    .acquire_timeout(Duration::from_secs(30))
    .idle_timeout(Duration::from_secs(600))
    .max_lifetime(Duration::from_secs(1800))
    .connect(&database_url)
    .await?;
```

### 2. ‰∫ãÂä°ÁÆ°ÁêÜ

```rust
// ‰øùÂ≠òÂ∑•‰ΩúÊµÅÊâßË°åÂíå‰∫ã‰ª∂Âú®‰∏Ä‰∏™‰∫ãÂä°‰∏≠
async fn save_workflow_with_event(
    &self,
    execution: &WorkflowExecution,
    event: WorkflowEvent,
) -> Result<(), StorageError> {
    let mut tx = self.pool.begin().await?;
    
    // ‰øùÂ≠òÊâßË°å
    sqlx::query("INSERT INTO workflow_executions ...")
        .execute(&mut tx)
        .await?;
    
    // ‰øùÂ≠ò‰∫ã‰ª∂
    sqlx::query("INSERT INTO workflow_events ...")
        .execute(&mut tx)
        .await?;
    
    tx.commit().await?;
    Ok(())
}
```

### 3. Á¥¢Âºï‰ºòÂåñ

```sql
-- Â∑•‰ΩúÊµÅÊü•ËØ¢‰ºòÂåñ
CREATE INDEX idx_workflows_type_state 
    ON workflow_executions(workflow_type, state);

CREATE INDEX idx_workflows_started_at 
    ON workflow_executions(started_at DESC);

-- ‰∫ã‰ª∂Êü•ËØ¢‰ºòÂåñ
CREATE INDEX idx_events_workflow_event 
    ON workflow_events(workflow_id, event_id);
```

### 4. Êï∞ÊçÆÂΩíÊ°£

```rust
/// ÂΩíÊ°£ÂÆåÊàêÁöÑÂ∑•‰ΩúÊµÅ
pub async fn archive_completed_workflows(
    &self,
    before: DateTime<Utc>,
) -> Result<usize, StorageError> {
    // ÁßªÂä®Âà∞ÂΩíÊ°£Ë°®
    let result = sqlx::query(
        r#"
        INSERT INTO workflow_executions_archive
        SELECT * FROM workflow_executions
        WHERE state IN ('Completed', 'Failed', 'Cancelled')
          AND completed_at < $1
        "#
    )
    .bind(before)
    .execute(&self.pool)
    .await?;
    
    let archived = result.rows_affected();
    
    // Âà†Èô§ÂéüÂßãËÆ∞ÂΩï
    sqlx::query(
        r#"
        DELETE FROM workflow_executions
        WHERE state IN ('Completed', 'Failed', 'Cancelled')
          AND completed_at < $1
        "#
    )
    .bind(before)
    .execute(&self.pool)
    .await?;
    
    Ok(archived as usize)
}
```

---

## üìö ÊÄªÁªì

### ÊåÅ‰πÖÂåñÂÖ≥ÈîÆÁÇπ

1. **‰∫ã‰ª∂Ê∫ØÊ∫ê**: ÈÄöËøá‰∫ã‰ª∂ÂéÜÂè≤ÈáçÂª∫Áä∂ÊÄÅ
2. **ÊÄßËÉΩ‰ºòÂåñ**: ËøûÊé•Ê±†„ÄÅÁ¥¢Âºï„ÄÅÊü•ËØ¢‰ºòÂåñ
3. **Êï∞ÊçÆ‰∏ÄËá¥ÊÄß**: ‰ΩøÁî®‰∫ãÂä°‰øùËØÅ
4. **Êâ©Â±ïÊÄß**: ÊîØÊåÅÂàÜÂ∫ìÂàÜË°®
5. **ÂΩíÊ°£Á≠ñÁï•**: ÂÆöÊúüÂΩíÊ°£ÂéÜÂè≤Êï∞ÊçÆ

### Rust vs Golang

- **Rust**: ÈúÄË¶ÅËá™Â∑±ÂÆûÁé∞Â≠òÂÇ®Â±ÇÔºàÊõ¥ÁÅµÊ¥ªÔºâ
- **Golang**: ‰ΩøÁî®Temporal ServiceÁöÑÊåÅ‰πÖÂåñÔºàÂºÄÁÆ±Âç≥Áî®Ôºâ

---

## üìö ‰∏ã‰∏ÄÊ≠•

- **ÈÉ®ÁΩ≤Á≠ñÁï•**: [Áîü‰∫ßÈÉ®ÁΩ≤](./deployment.md)
- **ÁõëÊéßÂëäË≠¶**: [ÂèØËßÇÊµãÊÄß](./monitoring.md)
- **ÊÄßËÉΩË∞É‰ºò**: [ÊÄßËÉΩ‰ºòÂåñ](./performance.md)

---

**ÊñáÊ°£ÁâàÊú¨**: 1.0.0  
**ÊúÄÂêéÊõ¥Êñ∞**: 2025-10-26  
**Áª¥Êä§ËÄÖ**: temporal-rust ÊñáÊ°£Âõ¢Èòü
