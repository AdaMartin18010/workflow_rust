# æŒä¹…åŒ–å®ç°

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†é˜è¿°Temporalçš„æŒä¹…åŒ–å±‚å®ç°ï¼ŒåŒ…æ‹¬ï¼š

- æŒä¹…åŒ–æ¶æ„
- äº‹ä»¶å­˜å‚¨
- Rust 1.90å®ç°
- Golangå®ç°å¯¹æ¯”
- å¤šç§å­˜å‚¨åç«¯
- æ€§èƒ½ä¼˜åŒ–

---

## ğŸ¯ æŒä¹…åŒ–æ ¸å¿ƒæ¦‚å¿µ

### ä¸ºä»€ä¹ˆéœ€è¦æŒä¹…åŒ–ï¼Ÿ

Temporalçš„æ ¸å¿ƒèƒ½åŠ›ä¾èµ–äºæŒä¹…åŒ–ï¼š

1. **æŒä¹…æ€§ (Durability)**: å·¥ä½œæµçŠ¶æ€åœ¨è¿›ç¨‹å´©æºƒåå¯æ¢å¤
2. **äº‹ä»¶æº¯æº (Event Sourcing)**: é€šè¿‡äº‹ä»¶å†å²é‡å»ºçŠ¶æ€
3. **ç¡®å®šæ€§é‡æ”¾ (Deterministic Replay)**: æ”¯æŒä»£ç æ›´æ–°
4. **å¯è§‚æµ‹æ€§**: æŸ¥è¯¢å†å²æ‰§è¡Œè®°å½•

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æŒä¹…åŒ–æ¶æ„                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Worker                  Temporal Service            Storage
  â”‚                           â”‚                        â”‚
  â”œâ”€ Execute Workflow â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                        â”‚
  â”‚                           â”œâ”€ Save Event â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
  â”‚                           â”‚                        â”‚
  â”‚  â—€â”€â”€â”€â”€ Crash â”€â”€â”€â”€â”€       â”‚                        â”‚
  â”‚                           â”‚                        â”‚
  â”œâ”€ Restart â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                        â”‚
  â”‚                           â”œâ”€ Load Events â—€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚                           â”‚                        â”‚
  â”œâ”€ Replay Events â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                        â”‚
  â”‚                           â”‚                        â”‚
  â””â”€ Continue Execution â”€â”€â”€â”€â”€â–¶â”‚                        â”‚
```

---

## ğŸ¦€ Rustå®ç°

### å­˜å‚¨æ¥å£å®šä¹‰

```rust
use async_trait::async_trait;
use serde::{Serialize, Deserialize};

/// å·¥ä½œæµå­˜å‚¨trait
#[async_trait]
pub trait WorkflowStorage: Send + Sync {
    /// ä¿å­˜å·¥ä½œæµæ‰§è¡Œ
    async fn save_workflow_execution(
        &self,
        execution: &WorkflowExecution,
        metadata: &WorkflowMetadata,
    ) -> Result<(), StorageError>;
    
    /// åŠ è½½å·¥ä½œæµæ‰§è¡Œ
    async fn load_workflow_execution(
        &self,
        workflow_id: &WorkflowId,
    ) -> Result<Option<WorkflowExecutionRecord>, StorageError>;
    
    /// è¿½åŠ äº‹ä»¶
    async fn append_event(
        &self,
        workflow_id: &WorkflowId,
        event: WorkflowEvent,
    ) -> Result<EventId, StorageError>;
    
    /// è·å–äº‹ä»¶å†å²
    async fn get_event_history(
        &self,
        workflow_id: &WorkflowId,
        from_event_id: Option<EventId>,
        limit: Option<usize>,
    ) -> Result<Vec<WorkflowEvent>, StorageError>;
    
    /// æ›´æ–°å·¥ä½œæµçŠ¶æ€
    async fn update_workflow_state(
        &self,
        workflow_id: &WorkflowId,
        state: WorkflowLifecycleState,
    ) -> Result<(), StorageError>;
    
    /// ä¿å­˜Activityå¿ƒè·³
    async fn save_activity_heartbeat(
        &self,
        activity_id: &ActivityId,
        details: serde_json::Value,
        timestamp: DateTime<Utc>,
    ) -> Result<(), StorageError>;
    
    /// æŸ¥è¯¢å·¥ä½œæµ
    async fn query_workflows(
        &self,
        query: WorkflowQuery,
    ) -> Result<Vec<WorkflowExecutionRecord>, StorageError>;
}

/// å·¥ä½œæµå…ƒæ•°æ®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WorkflowMetadata {
    pub workflow_type: String,
    pub task_queue: String,
    pub started_at: DateTime<Utc>,
    pub timeout_config: TimeoutConfig,
    pub tags: HashMap<String, String>,
}

/// å·¥ä½œæµæ‰§è¡Œè®°å½•
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WorkflowExecutionRecord {
    pub execution: WorkflowExecution,
    pub metadata: WorkflowMetadata,
    pub state: WorkflowLifecycleState,
    pub event_history: Vec<WorkflowEvent>,
    pub updated_at: DateTime<Utc>,
}

/// å·¥ä½œæµæŸ¥è¯¢
#[derive(Debug, Clone)]
pub struct WorkflowQuery {
    pub workflow_type: Option<String>,
    pub state: Option<WorkflowLifecycleState>,
    pub started_after: Option<DateTime<Utc>>,
    pub started_before: Option<DateTime<Utc>>,
    pub limit: Option<usize>,
    pub offset: Option<usize>,
}
```

### PostgreSQLå®ç°

```rust
use sqlx::{PgPool, Row};
use sqlx::postgres::PgRow;

/// PostgreSQLå­˜å‚¨å®ç°
pub struct PostgresWorkflowStorage {
    pool: PgPool,
}

impl PostgresWorkflowStorage {
    /// åˆ›å»ºæ–°å®ä¾‹
    pub async fn new(database_url: &str) -> Result<Self, StorageError> {
        let pool = PgPool::connect(database_url)
            .await
            .map_err(|e| StorageError::ConnectionError(e.to_string()))?;
        
        Ok(Self { pool })
    }
    
    /// åˆå§‹åŒ–æ•°æ®åº“schema
    pub async fn init_schema(&self) -> Result<(), StorageError> {
        sqlx::query(
            r#"
            CREATE TABLE IF NOT EXISTS workflow_executions (
                workflow_id TEXT PRIMARY KEY,
                run_id TEXT NOT NULL,
                workflow_type TEXT NOT NULL,
                task_queue TEXT NOT NULL,
                state TEXT NOT NULL,
                started_at TIMESTAMPTZ NOT NULL,
                updated_at TIMESTAMPTZ NOT NULL,
                completed_at TIMESTAMPTZ,
                metadata JSONB NOT NULL,
                UNIQUE(workflow_id, run_id)
            );
            
            CREATE TABLE IF NOT EXISTS workflow_events (
                id BIGSERIAL PRIMARY KEY,
                workflow_id TEXT NOT NULL,
                event_id BIGINT NOT NULL,
                event_type TEXT NOT NULL,
                event_data JSONB NOT NULL,
                timestamp TIMESTAMPTZ NOT NULL,
                UNIQUE(workflow_id, event_id)
            );
            
            CREATE INDEX IF NOT EXISTS idx_workflow_events_workflow_id 
                ON workflow_events(workflow_id, event_id);
            
            CREATE TABLE IF NOT EXISTS activity_heartbeats (
                activity_id TEXT PRIMARY KEY,
                workflow_id TEXT NOT NULL,
                details JSONB,
                timestamp TIMESTAMPTZ NOT NULL
            );
            
            CREATE INDEX IF NOT EXISTS idx_activity_heartbeats_workflow_id 
                ON activity_heartbeats(workflow_id);
            "#
        )
        .execute(&self.pool)
        .await
        .map_err(|e| StorageError::QueryError(e.to_string()))?;
        
        Ok(())
    }
}

#[async_trait]
impl WorkflowStorage for PostgresWorkflowStorage {
    async fn save_workflow_execution(
        &self,
        execution: &WorkflowExecution,
        metadata: &WorkflowMetadata,
    ) -> Result<(), StorageError> {
        sqlx::query(
            r#"
            INSERT INTO workflow_executions (
                workflow_id, run_id, workflow_type, task_queue, 
                state, started_at, updated_at, metadata
            )
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
            ON CONFLICT (workflow_id) 
            DO UPDATE SET 
                run_id = EXCLUDED.run_id,
                updated_at = EXCLUDED.updated_at,
                metadata = EXCLUDED.metadata
            "#
        )
        .bind(&execution.workflow_id.as_str())
        .bind(&execution.run_id.to_string())
        .bind(&metadata.workflow_type)
        .bind(&metadata.task_queue)
        .bind("Running")
        .bind(metadata.started_at)
        .bind(Utc::now())
        .bind(serde_json::to_value(metadata).unwrap())
        .execute(&self.pool)
        .await
        .map_err(|e| StorageError::QueryError(e.to_string()))?;
        
        Ok(())
    }
    
    async fn load_workflow_execution(
        &self,
        workflow_id: &WorkflowId,
    ) -> Result<Option<WorkflowExecutionRecord>, StorageError> {
        let row = sqlx::query(
            r#"
            SELECT workflow_id, run_id, workflow_type, task_queue, 
                   state, started_at, updated_at, metadata
            FROM workflow_executions
            WHERE workflow_id = $1
            "#
        )
        .bind(workflow_id.as_str())
        .fetch_optional(&self.pool)
        .await
        .map_err(|e| StorageError::QueryError(e.to_string()))?;
        
        if let Some(row) = row {
            let execution = WorkflowExecution::with_run_id(
                WorkflowId::new(row.get::<String, _>("workflow_id")),
                RunId::parse(&row.get::<String, _>("run_id")).unwrap(),
            );
            
            let metadata: WorkflowMetadata = serde_json::from_value(
                row.get::<serde_json::Value, _>("metadata")
            ).unwrap();
            
            // åŠ è½½äº‹ä»¶å†å²
            let event_history = self.get_event_history(workflow_id, None, None).await?;
            
            Ok(Some(WorkflowExecutionRecord {
                execution,
                metadata,
                state: parse_state(row.get("state")),
                event_history,
                updated_at: row.get("updated_at"),
            }))
        } else {
            Ok(None)
        }
    }
    
    async fn append_event(
        &self,
        workflow_id: &WorkflowId,
        event: WorkflowEvent,
    ) -> Result<EventId, StorageError> {
        let row = sqlx::query(
            r#"
            INSERT INTO workflow_events (
                workflow_id, event_id, event_type, event_data, timestamp
            )
            VALUES ($1, $2, $3, $4, $5)
            RETURNING id
            "#
        )
        .bind(workflow_id.as_str())
        .bind(event.event_id.0 as i64)
        .bind(format!("{:?}", event.event_type))
        .bind(serde_json::to_value(&event.event_type).unwrap())
        .bind(event.timestamp)
        .fetch_one(&self.pool)
        .await
        .map_err(|e| StorageError::QueryError(e.to_string()))?;
        
        Ok(EventId(row.get::<i64, _>("id") as u64))
    }
    
    async fn get_event_history(
        &self,
        workflow_id: &WorkflowId,
        from_event_id: Option<EventId>,
        limit: Option<usize>,
    ) -> Result<Vec<WorkflowEvent>, StorageError> {
        let mut query = String::from(
            "SELECT event_id, event_type, event_data, timestamp FROM workflow_events WHERE workflow_id = $1"
        );
        
        if from_event_id.is_some() {
            query.push_str(" AND event_id >= $2");
        }
        
        query.push_str(" ORDER BY event_id ASC");
        
        if let Some(limit) = limit {
            query.push_str(&format!(" LIMIT {}", limit));
        }
        
        let mut sql_query = sqlx::query(&query).bind(workflow_id.as_str());
        
        if let Some(from_id) = from_event_id {
            sql_query = sql_query.bind(from_id.0 as i64);
        }
        
        let rows = sql_query
            .fetch_all(&self.pool)
            .await
            .map_err(|e| StorageError::QueryError(e.to_string()))?;
        
        let events = rows
            .into_iter()
            .map(|row| {
                WorkflowEvent {
                    event_id: EventId(row.get::<i64, _>("event_id") as u64),
                    timestamp: row.get("timestamp"),
                    event_type: serde_json::from_value(
                        row.get::<serde_json::Value, _>("event_data")
                    ).unwrap(),
                }
            })
            .collect();
        
        Ok(events)
    }
    
    async fn update_workflow_state(
        &self,
        workflow_id: &WorkflowId,
        state: WorkflowLifecycleState,
    ) -> Result<(), StorageError> {
        sqlx::query(
            r#"
            UPDATE workflow_executions 
            SET state = $1, updated_at = $2
            WHERE workflow_id = $3
            "#
        )
        .bind(format!("{:?}", state))
        .bind(Utc::now())
        .bind(workflow_id.as_str())
        .execute(&self.pool)
        .await
        .map_err(|e| StorageError::QueryError(e.to_string()))?;
        
        Ok(())
    }
    
    async fn save_activity_heartbeat(
        &self,
        activity_id: &ActivityId,
        details: serde_json::Value,
        timestamp: DateTime<Utc>,
    ) -> Result<(), StorageError> {
        sqlx::query(
            r#"
            INSERT INTO activity_heartbeats (activity_id, workflow_id, details, timestamp)
            VALUES ($1, $2, $3, $4)
            ON CONFLICT (activity_id)
            DO UPDATE SET details = EXCLUDED.details, timestamp = EXCLUDED.timestamp
            "#
        )
        .bind(activity_id.as_str())
        .bind("unknown")  // å®é™…åº”è¯¥ä»activity_idæå–
        .bind(details)
        .bind(timestamp)
        .execute(&self.pool)
        .await
        .map_err(|e| StorageError::QueryError(e.to_string()))?;
        
        Ok(())
    }
    
    async fn query_workflows(
        &self,
        query: WorkflowQuery,
    ) -> Result<Vec<WorkflowExecutionRecord>, StorageError> {
        // æ„å»ºåŠ¨æ€æŸ¥è¯¢
        let mut sql = String::from("SELECT * FROM workflow_executions WHERE 1=1");
        let mut bindings: Vec<String> = vec![];
        
        if let Some(workflow_type) = &query.workflow_type {
            sql.push_str(&format!(" AND workflow_type = ${}", bindings.len() + 1));
            bindings.push(workflow_type.clone());
        }
        
        if let Some(state) = &query.state {
            sql.push_str(&format!(" AND state = ${}", bindings.len() + 1));
            bindings.push(format!("{:?}", state));
        }
        
        sql.push_str(" ORDER BY started_at DESC");
        
        if let Some(limit) = query.limit {
            sql.push_str(&format!(" LIMIT {}", limit));
        }
        
        if let Some(offset) = query.offset {
            sql.push_str(&format!(" OFFSET {}", offset));
        }
        
        // æ‰§è¡ŒæŸ¥è¯¢å¹¶è½¬æ¢ç»“æœ
        // (ç®€åŒ–å®ç°)
        Ok(vec![])
    }
}
```

### å†…å­˜å­˜å‚¨å®ç°ï¼ˆæµ‹è¯•ç”¨ï¼‰

```rust
use std::collections::HashMap;
use tokio::sync::RwLock;

/// å†…å­˜å­˜å‚¨å®ç°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
pub struct InMemoryWorkflowStorage {
    executions: Arc<RwLock<HashMap<WorkflowId, WorkflowExecutionRecord>>>,
    events: Arc<RwLock<HashMap<WorkflowId, Vec<WorkflowEvent>>>>,
}

impl InMemoryWorkflowStorage {
    pub fn new() -> Self {
        Self {
            executions: Arc::new(RwLock::new(HashMap::new())),
            events: Arc::new(RwLock::new(HashMap::new())),
        }
    }
}

#[async_trait]
impl WorkflowStorage for InMemoryWorkflowStorage {
    async fn save_workflow_execution(
        &self,
        execution: &WorkflowExecution,
        metadata: &WorkflowMetadata,
    ) -> Result<(), StorageError> {
        let record = WorkflowExecutionRecord {
            execution: execution.clone(),
            metadata: metadata.clone(),
            state: WorkflowLifecycleState::Running,
            event_history: vec![],
            updated_at: Utc::now(),
        };
        
        self.executions
            .write()
            .await
            .insert(execution.workflow_id.clone(), record);
        
        Ok(())
    }
    
    async fn load_workflow_execution(
        &self,
        workflow_id: &WorkflowId,
    ) -> Result<Option<WorkflowExecutionRecord>, StorageError> {
        let executions = self.executions.read().await;
        Ok(executions.get(workflow_id).cloned())
    }
    
    async fn append_event(
        &self,
        workflow_id: &WorkflowId,
        event: WorkflowEvent,
    ) -> Result<EventId, StorageError> {
        let mut events = self.events.write().await;
        let workflow_events = events.entry(workflow_id.clone()).or_insert_with(Vec::new);
        workflow_events.push(event.clone());
        Ok(event.event_id)
    }
    
    async fn get_event_history(
        &self,
        workflow_id: &WorkflowId,
        from_event_id: Option<EventId>,
        limit: Option<usize>,
    ) -> Result<Vec<WorkflowEvent>, StorageError> {
        let events = self.events.read().await;
        
        if let Some(workflow_events) = events.get(workflow_id) {
            let mut filtered: Vec<_> = workflow_events.iter()
                .filter(|e| {
                    if let Some(from_id) = from_event_id {
                        e.event_id >= from_id
                    } else {
                        true
                    }
                })
                .cloned()
                .collect();
            
            if let Some(limit) = limit {
                filtered.truncate(limit);
            }
            
            Ok(filtered)
        } else {
            Ok(vec![])
        }
    }
    
    async fn update_workflow_state(
        &self,
        workflow_id: &WorkflowId,
        state: WorkflowLifecycleState,
    ) -> Result<(), StorageError> {
        let mut executions = self.executions.write().await;
        
        if let Some(record) = executions.get_mut(workflow_id) {
            record.state = state;
            record.updated_at = Utc::now();
        }
        
        Ok(())
    }
    
    // å…¶ä»–æ–¹æ³•å®ç°...
    async fn save_activity_heartbeat(
        &self,
        _activity_id: &ActivityId,
        _details: serde_json::Value,
        _timestamp: DateTime<Utc>,
    ) -> Result<(), StorageError> {
        Ok(())
    }
    
    async fn query_workflows(
        &self,
        _query: WorkflowQuery,
    ) -> Result<Vec<WorkflowExecutionRecord>, StorageError> {
        let executions = self.executions.read().await;
        Ok(executions.values().cloned().collect())
    }
}
```

---

## ğŸ¹ Golangå®ç°å¯¹æ¯”

### Temporal Go SDKæŒä¹…åŒ–

Golangçš„Temporal SDKä½¿ç”¨Temporal Serviceçš„æŒä¹…åŒ–å±‚ï¼Œä¸éœ€è¦ç›´æ¥å®ç°å­˜å‚¨æ¥å£ã€‚

```go
// Temporal Serviceé…ç½®ï¼ˆç®¡ç†å‘˜æ“ä½œï¼‰
import (
    "go.temporal.io/server/common/config"
)

// PostgreSQLé…ç½®
cfg := &config.Persistence{
    DefaultStore: "default",
    DataStores: map[string]config.DataStore{
        "default": {
            SQL: &config.SQL{
                PluginName:   "postgres",
                DatabaseName: "temporal",
                ConnectAddr:  "localhost:5432",
                User:         "temporal",
                Password:     "temporal",
            },
        },
    },
}
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### 1. è¿æ¥æ± ç®¡ç†

```rust
// âœ… å¥½: ä½¿ç”¨è¿æ¥æ± 
let pool = PgPoolOptions::new()
    .max_connections(20)
    .min_connections(5)
    .acquire_timeout(Duration::from_secs(30))
    .idle_timeout(Duration::from_secs(600))
    .max_lifetime(Duration::from_secs(1800))
    .connect(&database_url)
    .await?;
```

### 2. äº‹åŠ¡ç®¡ç†

```rust
// ä¿å­˜å·¥ä½œæµæ‰§è¡Œå’Œäº‹ä»¶åœ¨ä¸€ä¸ªäº‹åŠ¡ä¸­
async fn save_workflow_with_event(
    &self,
    execution: &WorkflowExecution,
    event: WorkflowEvent,
) -> Result<(), StorageError> {
    let mut tx = self.pool.begin().await?;
    
    // ä¿å­˜æ‰§è¡Œ
    sqlx::query("INSERT INTO workflow_executions ...")
        .execute(&mut tx)
        .await?;
    
    // ä¿å­˜äº‹ä»¶
    sqlx::query("INSERT INTO workflow_events ...")
        .execute(&mut tx)
        .await?;
    
    tx.commit().await?;
    Ok(())
}
```

### 3. ç´¢å¼•ä¼˜åŒ–

```sql
-- å·¥ä½œæµæŸ¥è¯¢ä¼˜åŒ–
CREATE INDEX idx_workflows_type_state 
    ON workflow_executions(workflow_type, state);

CREATE INDEX idx_workflows_started_at 
    ON workflow_executions(started_at DESC);

-- äº‹ä»¶æŸ¥è¯¢ä¼˜åŒ–
CREATE INDEX idx_events_workflow_event 
    ON workflow_events(workflow_id, event_id);
```

### 4. æ•°æ®å½’æ¡£

```rust
/// å½’æ¡£å®Œæˆçš„å·¥ä½œæµ
pub async fn archive_completed_workflows(
    &self,
    before: DateTime<Utc>,
) -> Result<usize, StorageError> {
    // ç§»åŠ¨åˆ°å½’æ¡£è¡¨
    let result = sqlx::query(
        r#"
        INSERT INTO workflow_executions_archive
        SELECT * FROM workflow_executions
        WHERE state IN ('Completed', 'Failed', 'Cancelled')
          AND completed_at < $1
        "#
    )
    .bind(before)
    .execute(&self.pool)
    .await?;
    
    let archived = result.rows_affected();
    
    // åˆ é™¤åŸå§‹è®°å½•
    sqlx::query(
        r#"
        DELETE FROM workflow_executions
        WHERE state IN ('Completed', 'Failed', 'Cancelled')
          AND completed_at < $1
        "#
    )
    .bind(before)
    .execute(&self.pool)
    .await?;
    
    Ok(archived as usize)
}
```

---

## ğŸ“š æ€»ç»“

### æŒä¹…åŒ–å…³é”®ç‚¹

1. **äº‹ä»¶æº¯æº**: é€šè¿‡äº‹ä»¶å†å²é‡å»ºçŠ¶æ€
2. **æ€§èƒ½ä¼˜åŒ–**: è¿æ¥æ± ã€ç´¢å¼•ã€æŸ¥è¯¢ä¼˜åŒ–
3. **æ•°æ®ä¸€è‡´æ€§**: ä½¿ç”¨äº‹åŠ¡ä¿è¯
4. **æ‰©å±•æ€§**: æ”¯æŒåˆ†åº“åˆ†è¡¨
5. **å½’æ¡£ç­–ç•¥**: å®šæœŸå½’æ¡£å†å²æ•°æ®

### Rust vs Golang

- **Rust**: éœ€è¦è‡ªå·±å®ç°å­˜å‚¨å±‚ï¼ˆæ›´çµæ´»ï¼‰
- **Golang**: ä½¿ç”¨Temporal Serviceçš„æŒä¹…åŒ–ï¼ˆå¼€ç®±å³ç”¨ï¼‰

---

## ğŸ“š ä¸‹ä¸€æ­¥

- **éƒ¨ç½²ç­–ç•¥**: [ç”Ÿäº§éƒ¨ç½²](./deployment.md)
- **ç›‘æ§å‘Šè­¦**: [å¯è§‚æµ‹æ€§](./monitoring.md)
- **æ€§èƒ½è°ƒä¼˜**: [æ€§èƒ½ä¼˜åŒ–](./performance.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0.0  
**æœ€åæ›´æ–°**: 2025-10-26  
**ç»´æŠ¤è€…**: temporal-rust æ–‡æ¡£å›¢é˜Ÿ
