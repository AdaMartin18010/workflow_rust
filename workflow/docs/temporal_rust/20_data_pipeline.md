# æ•°æ®ç®¡é“ç¤ºä¾‹

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›æ•°æ®ç®¡é“ç›¸å…³çš„ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ï¼š

- ETLï¼ˆExtract, Transform, Loadï¼‰æµç¨‹
- æµå¼æ•°æ®å¤„ç†
- æ•°æ®åŒæ­¥
- æ‰¹é‡å¯¼å…¥
- æ•°æ®éªŒè¯å’Œæ¸…æ´—
- Rust + Golangå¹¶åˆ—å¯¹æ¯”

---

## ğŸ”„ ETLç®¡é“ç¤ºä¾‹

### åœºæ™¯ï¼šä»å¤šä¸ªæ•°æ®æºæå–ã€è½¬æ¢å¹¶åŠ è½½æ•°æ®

#### Rustå®ç°

```rust
use temporal_rust::*;
use serde::{Serialize, Deserialize};

// ========================================
// ETLç®¡é“å·¥ä½œæµ
// ========================================

#[derive(Serialize, Deserialize)]
pub struct ETLInput {
    pub pipeline_id: String,
    pub sources: Vec<DataSource>,
    pub target: DataTarget,
    pub batch_size: usize,
}

#[derive(Serialize, Deserialize, Clone)]
pub struct DataSource {
    pub source_id: String,
    pub source_type: String, // "database", "api", "file"
    pub connection_string: String,
}

#[derive(Serialize, Deserialize, Clone)]
pub struct DataTarget {
    pub target_type: String,
    pub connection_string: String,
}

#[derive(Serialize, Deserialize)]
pub struct ETLOutput {
    pub pipeline_id: String,
    pub total_records: usize,
    pub successful_records: usize,
    pub failed_records: usize,
    pub duration_seconds: u64,
}

pub struct ETLPipelineWorkflow;

impl Workflow for ETLPipelineWorkflow {
    type Input = ETLInput;
    type Output = ETLOutput;
    
    fn name() -> &'static str {
        "ETLPipeline"
    }
    
    async fn execute(
        ctx: WorkflowContext,
        input: Self::Input,
    ) -> Result<Self::Output, WorkflowError> {
        let start_time = ctx.now();
        
        tracing::info!(
            pipeline_id = %input.pipeline_id,
            sources = input.sources.len(),
            "Starting ETL pipeline"
        );
        
        let mut total_records = 0;
        let mut successful_records = 0;
        let mut failed_records = 0;
        
        // 1. Extracté˜¶æ®µ - å¹¶è¡Œä»å¤šä¸ªæºæå–æ•°æ®
        let extract_futures: Vec<_> = input.sources.iter().map(|source| {
            ctx.execute_activity::<ExtractDataActivity>(
                ExtractInput {
                    source: source.clone(),
                    batch_size: input.batch_size,
                },
                ActivityOptions {
                    start_to_close_timeout: Some(Duration::from_secs(600)),
                    ..Default::default()
                },
            )
        }).collect();
        
        let extract_results = futures::future::join_all(extract_futures).await;
        
        // æ”¶é›†æ‰€æœ‰æå–çš„æ•°æ®
        let mut all_records = Vec::new();
        for result in extract_results {
            match result {
                Ok(data) => {
                    total_records += data.records.len();
                    all_records.extend(data.records);
                }
                Err(e) => {
                    tracing::error!(error = ?e, "Extract failed");
                    return Err(e);
                }
            }
        }
        
        tracing::info!(
            total_records = total_records,
            "Extraction completed"
        );
        
        // 2. Transformé˜¶æ®µ - åˆ†æ‰¹è½¬æ¢æ•°æ®
        let mut transformed_records = Vec::new();
        
        for chunk in all_records.chunks(input.batch_size) {
            let transform_result = ctx.execute_activity::<TransformDataActivity>(
                TransformInput {
                    records: chunk.to_vec(),
                },
                ActivityOptions {
                    start_to_close_timeout: Some(Duration::from_secs(300)),
                    retry_policy: Some(RetryPolicy {
                        max_attempts: Some(3),
                        initial_interval: Duration::from_secs(1),
                        max_interval: Duration::from_secs(10),
                        backoff_coefficient: 2.0,
                        non_retryable_error_types: vec![],
                    }),
                    ..Default::default()
                },
            ).await?;
            
            transformed_records.extend(transform_result.records);
            
            // å‘é€å¿ƒè·³ï¼ˆé•¿æ—¶é—´è¿è¡Œï¼‰
            ctx.record_heartbeat(serde_json::json!({
                "phase": "transform",
                "processed": transformed_records.len(),
                "total": total_records
            })).await;
        }
        
        tracing::info!(
            transformed_records = transformed_records.len(),
            "Transformation completed"
        );
        
        // 3. Loadé˜¶æ®µ - åˆ†æ‰¹åŠ è½½åˆ°ç›®æ ‡
        for chunk in transformed_records.chunks(input.batch_size) {
            match ctx.execute_activity::<LoadDataActivity>(
                LoadInput {
                    target: input.target.clone(),
                    records: chunk.to_vec(),
                },
                ActivityOptions {
                    start_to_close_timeout: Some(Duration::from_secs(300)),
                    retry_policy: Some(RetryPolicy {
                        max_attempts: Some(5),
                        initial_interval: Duration::from_secs(2),
                        max_interval: Duration::from_secs(30),
                        backoff_coefficient: 2.0,
                        non_retryable_error_types: vec!["ValidationError"],
                    }),
                    ..Default::default()
                },
            ).await {
                Ok(result) => {
                    successful_records += result.loaded_count;
                }
                Err(e) => {
                    tracing::error!(error = ?e, "Load failed for batch");
                    failed_records += chunk.len();
                    // ç»§ç»­å¤„ç†å…¶ä»–æ‰¹æ¬¡
                }
            }
            
            // å‘é€å¿ƒè·³
            ctx.record_heartbeat(serde_json::json!({
                "phase": "load",
                "loaded": successful_records,
                "total": total_records
            })).await;
        }
        
        let end_time = ctx.now();
        let duration = end_time.signed_duration_since(start_time);
        
        tracing::info!(
            successful = successful_records,
            failed = failed_records,
            duration_secs = duration.num_seconds(),
            "ETL pipeline completed"
        );
        
        Ok(ETLOutput {
            pipeline_id: input.pipeline_id,
            total_records,
            successful_records,
            failed_records,
            duration_seconds: duration.num_seconds() as u64,
        })
    }
}

// ========================================
// Activityå®šä¹‰
// ========================================

pub struct ExtractDataActivity;

#[derive(Serialize, Deserialize)]
pub struct ExtractInput {
    pub source: DataSource,
    pub batch_size: usize,
}

#[derive(Serialize, Deserialize)]
pub struct ExtractOutput {
    pub records: Vec<RawRecord>,
}

#[derive(Serialize, Deserialize, Clone)]
pub struct RawRecord {
    pub id: String,
    pub data: serde_json::Value,
}

impl Activity for ExtractDataActivity {
    type Input = ExtractInput;
    type Output = ExtractOutput;
    
    fn name() -> &'static str {
        "ExtractData"
    }
    
    async fn execute(
        ctx: ActivityContext,
        input: Self::Input,
    ) -> Result<Self::Output, ActivityError> {
        tracing::info!(
            source_id = %input.source.source_id,
            source_type = %input.source.source_type,
            "Extracting data"
        );
        
        // æ ¹æ®æºç±»å‹æå–æ•°æ®
        let records = match input.source.source_type.as_str() {
            "database" => extract_from_database(&input.source, input.batch_size).await?,
            "api" => extract_from_api(&input.source, input.batch_size).await?,
            "file" => extract_from_file(&input.source, input.batch_size).await?,
            _ => return Err(ActivityError::invalid_input("Unsupported source type")),
        };
        
        // å®šæœŸå‘é€å¿ƒè·³
        ctx.record_heartbeat(serde_json::json!({
            "extracted": records.len()
        })).await;
        
        Ok(ExtractOutput { records })
    }
}

pub struct TransformDataActivity;

#[derive(Serialize, Deserialize)]
pub struct TransformInput {
    pub records: Vec<RawRecord>,
}

#[derive(Serialize, Deserialize)]
pub struct TransformOutput {
    pub records: Vec<TransformedRecord>,
}

#[derive(Serialize, Deserialize, Clone)]
pub struct TransformedRecord {
    pub id: String,
    pub data: serde_json::Value,
    pub metadata: RecordMetadata,
}

#[derive(Serialize, Deserialize, Clone)]
pub struct RecordMetadata {
    pub processed_at: chrono::DateTime<chrono::Utc>,
    pub version: String,
}

impl Activity for TransformDataActivity {
    type Input = TransformInput;
    type Output = TransformOutput;
    
    fn name() -> &'static str {
        "TransformData"
    }
    
    async fn execute(
        _ctx: ActivityContext,
        input: Self::Input,
    ) -> Result<Self::Output, ActivityError> {
        tracing::info!(
            records = input.records.len(),
            "Transforming data"
        );
        
        let mut transformed_records = Vec::new();
        
        for record in input.records {
            // æ•°æ®è½¬æ¢é€»è¾‘
            let transformed = transform_record(record)?;
            transformed_records.push(transformed);
        }
        
        Ok(TransformOutput {
            records: transformed_records,
        })
    }
}

pub struct LoadDataActivity;

#[derive(Serialize, Deserialize)]
pub struct LoadInput {
    pub target: DataTarget,
    pub records: Vec<TransformedRecord>,
}

#[derive(Serialize, Deserialize)]
pub struct LoadOutput {
    pub loaded_count: usize,
}

impl Activity for LoadDataActivity {
    type Input = LoadInput;
    type Output = LoadOutput;
    
    fn name() -> &'static str {
        "LoadData"
    }
    
    async fn execute(
        ctx: ActivityContext,
        input: Self::Input,
    ) -> Result<Self::Output, ActivityError> {
        tracing::info!(
            target_type = %input.target.target_type,
            records = input.records.len(),
            "Loading data"
        );
        
        // æ‰¹é‡å†™å…¥ç›®æ ‡
        let loaded_count = match input.target.target_type.as_str() {
            "database" => load_to_database(&input.target, &input.records).await?,
            "warehouse" => load_to_warehouse(&input.target, &input.records).await?,
            "file" => load_to_file(&input.target, &input.records).await?,
            _ => return Err(ActivityError::invalid_input("Unsupported target type")),
        };
        
        ctx.record_heartbeat(serde_json::json!({
            "loaded": loaded_count
        })).await;
        
        Ok(LoadOutput { loaded_count })
    }
}

// è¾…åŠ©å‡½æ•°
async fn extract_from_database(
    source: &DataSource,
    batch_size: usize,
) -> Result<Vec<RawRecord>, ActivityError> {
    // å®é™…å®ç°ï¼šè¿æ¥æ•°æ®åº“å¹¶æŸ¥è¯¢
    Ok(vec![])
}

async fn extract_from_api(
    source: &DataSource,
    batch_size: usize,
) -> Result<Vec<RawRecord>, ActivityError> {
    // å®é™…å®ç°ï¼šè°ƒç”¨APIå¹¶è·å–æ•°æ®
    Ok(vec![])
}

async fn extract_from_file(
    source: &DataSource,
    batch_size: usize,
) -> Result<Vec<RawRecord>, ActivityError> {
    // å®é™…å®ç°ï¼šè¯»å–æ–‡ä»¶
    Ok(vec![])
}

fn transform_record(record: RawRecord) -> Result<TransformedRecord, ActivityError> {
    // å®é™…å®ç°ï¼šæ•°æ®è½¬æ¢é€»è¾‘
    Ok(TransformedRecord {
        id: record.id,
        data: record.data,
        metadata: RecordMetadata {
            processed_at: chrono::Utc::now(),
            version: "1.0".to_string(),
        },
    })
}

async fn load_to_database(
    target: &DataTarget,
    records: &[TransformedRecord],
) -> Result<usize, ActivityError> {
    // å®é™…å®ç°ï¼šæ‰¹é‡æ’å…¥æ•°æ®åº“
    Ok(records.len())
}

async fn load_to_warehouse(
    target: &DataTarget,
    records: &[TransformedRecord],
) -> Result<usize, ActivityError> {
    // å®é™…å®ç°ï¼šåŠ è½½åˆ°æ•°æ®ä»“åº“
    Ok(records.len())
}

async fn load_to_file(
    target: &DataTarget,
    records: &[TransformedRecord],
) -> Result<usize, ActivityError> {
    // å®é™…å®ç°ï¼šå†™å…¥æ–‡ä»¶
    Ok(records.len())
}
```

#### Golangå¯¹æ¯”

```go
package workflows

import (
    "go.temporal.io/sdk/workflow"
)

type ETLInput struct {
    PipelineID string
    Sources    []DataSource
    Target     DataTarget
    BatchSize  int
}

type ETLOutput struct {
    PipelineID        string
    TotalRecords      int
    SuccessfulRecords int
    FailedRecords     int
    DurationSeconds   int64
}

func ETLPipelineWorkflow(ctx workflow.Context, input ETLInput) (ETLOutput, error) {
    startTime := workflow.Now(ctx)
    logger := workflow.GetLogger(ctx)
    
    logger.Info("Starting ETL pipeline", "pipeline_id", input.PipelineID)
    
    var totalRecords, successfulRecords, failedRecords int
    
    // 1. Extracté˜¶æ®µ - å¹¶è¡Œæå–
    var extractFutures []workflow.Future
    for _, source := range input.Sources {
        future := workflow.ExecuteActivity(ctx, ExtractDataActivity, source, input.BatchSize)
        extractFutures = append(extractFutures, future)
    }
    
    var allRecords []RawRecord
    for _, future := range extractFutures {
        var extractOutput ExtractOutput
        err := future.Get(ctx, &extractOutput)
        if err != nil {
            return ETLOutput{}, err
        }
        totalRecords += len(extractOutput.Records)
        allRecords = append(allRecords, extractOutput.Records...)
    }
    
    logger.Info("Extraction completed", "total_records", totalRecords)
    
    // 2. Transformé˜¶æ®µ
    var transformedRecords []TransformedRecord
    for i := 0; i < len(allRecords); i += input.BatchSize {
        end := i + input.BatchSize
        if end > len(allRecords) {
            end = len(allRecords)
        }
        chunk := allRecords[i:end]
        
        var transformOutput TransformOutput
        err := workflow.ExecuteActivity(ctx, TransformDataActivity, chunk).Get(ctx, &transformOutput)
        if err != nil {
            return ETLOutput{}, err
        }
        
        transformedRecords = append(transformedRecords, transformOutput.Records...)
        
        // è®°å½•å¿ƒè·³
        workflow.RecordHeartbeat(ctx, map[string]interface{}{
            "phase":     "transform",
            "processed": len(transformedRecords),
            "total":     totalRecords,
        })
    }
    
    // 3. Loadé˜¶æ®µ
    for i := 0; i < len(transformedRecords); i += input.BatchSize {
        end := i + input.BatchSize
        if end > len(transformedRecords) {
            end = len(transformedRecords)
        }
        chunk := transformedRecords[i:end]
        
        var loadOutput LoadOutput
        err := workflow.ExecuteActivity(ctx, LoadDataActivity, input.Target, chunk).Get(ctx, &loadOutput)
        if err != nil {
            logger.Error("Load failed for batch", "error", err)
            failedRecords += len(chunk)
            continue
        }
        
        successfulRecords += loadOutput.LoadedCount
    }
    
    duration := workflow.Now(ctx).Sub(startTime)
    
    return ETLOutput{
        PipelineID:        input.PipelineID,
        TotalRecords:      totalRecords,
        SuccessfulRecords: successfulRecords,
        FailedRecords:     failedRecords,
        DurationSeconds:   int64(duration.Seconds()),
    }, nil
}
```

---

## ğŸŒŠ æµå¼æ•°æ®å¤„ç†ç¤ºä¾‹

### åœºæ™¯ï¼šå®æ—¶æ•°æ®æµå¤„ç†

#### Rustå®ç°1

```rust
use temporal_rust::*;

#[derive(Serialize, Deserialize)]
pub struct StreamProcessingInput {
    pub stream_id: String,
    pub source_topic: String,
    pub target_topic: String,
    pub window_size_seconds: u64,
}

#[derive(Serialize, Deserialize)]
pub struct StreamProcessingOutput {
    pub stream_id: String,
    pub total_events: usize,
    pub processed_windows: usize,
}

pub struct StreamProcessingWorkflow;

impl Workflow for StreamProcessingWorkflow {
    type Input = StreamProcessingInput;
    type Output = StreamProcessingOutput;
    
    fn name() -> &'static str {
        "StreamProcessing"
    }
    
    async fn execute(
        ctx: WorkflowContext,
        input: Self::Input,
    ) -> Result<Self::Output, WorkflowError> {
        tracing::info!(
            stream_id = %input.stream_id,
            "Starting stream processing workflow"
        );
        
        let mut total_events = 0;
        let mut processed_windows = 0;
        let window_duration = Duration::from_secs(input.window_size_seconds);
        
        loop {
            // 1. è¯»å–æ—¶é—´çª—å£å†…çš„äº‹ä»¶
            let events = ctx.execute_activity::<ReadStreamEventsActivity>(
                ReadStreamInput {
                    topic: input.source_topic.clone(),
                    window_size: window_duration,
                },
                ActivityOptions {
                    start_to_close_timeout: Some(Duration::from_secs(60)),
                    ..Default::default()
                },
            ).await?;
            
            if events.events.is_empty() {
                // æ²¡æœ‰æ›´å¤šäº‹ä»¶ï¼Œç­‰å¾…
                ctx.sleep(window_duration).await;
                continue;
            }
            
            total_events += events.events.len();
            
            // 2. å¤„ç†äº‹ä»¶
            let processed = ctx.execute_activity::<ProcessStreamEventsActivity>(
                ProcessStreamInput {
                    events: events.events,
                },
                ActivityOptions::default(),
            ).await?;
            
            // 3. å†™å…¥ç»“æœ
            ctx.execute_activity::<WriteStreamResultsActivity>(
                WriteStreamInput {
                    topic: input.target_topic.clone(),
                    results: processed.results,
                },
                ActivityOptions::default(),
            ).await?;
            
            processed_windows += 1;
            
            // æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»§ç»­ï¼ˆé€šè¿‡Signalæ§åˆ¶ï¼‰
            if let Some(stop_signal) = ctx.try_receive_signal::<StopStreamingSignal>().await {
                tracing::info!("Received stop signal, terminating stream");
                break;
            }
            
            // Continue As Newä»¥é¿å…å†å²è¿‡å¤§
            if processed_windows >= 1000 {
                return ctx.continue_as_new(StreamProcessingInput {
                    stream_id: input.stream_id,
                    source_topic: input.source_topic,
                    target_topic: input.target_topic,
                    window_size_seconds: input.window_size_seconds,
                });
            }
        }
        
        Ok(StreamProcessingOutput {
            stream_id: input.stream_id,
            total_events,
            processed_windows,
        })
    }
}

#[derive(Serialize, Deserialize)]
pub struct StopStreamingSignal;

impl Signal for StopStreamingSignal {
    fn name() -> &'static str {
        "stop_streaming"
    }
}
```

---

## ğŸ“š æ€»ç»“

### ETLç®¡é“ä¼˜åŠ¿

1. **å¯é æ€§**: è‡ªåŠ¨é‡è¯•å’Œé”™è¯¯å¤„ç†
2. **å¯è§‚æµ‹æ€§**: è¯¦ç»†çš„è¿›åº¦è¿½è¸ª
3. **å¯æ‰©å±•æ€§**: å¹¶è¡Œå¤„ç†å¤šä¸ªæ•°æ®æº
4. **å®¹é”™æ€§**: éƒ¨åˆ†å¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹

### æµå¼å¤„ç†ä¼˜åŠ¿

1. **å®æ—¶æ€§**: æŒç»­å¤„ç†æ•°æ®æµ
2. **é•¿æ—¶é—´è¿è¡Œ**: Continue As Newé¿å…å†å²è¿‡å¤§
3. **å¯æ§åˆ¶**: é€šè¿‡SignalåŠ¨æ€æ§åˆ¶
4. **æ—¶é—´çª—å£**: æ”¯æŒçª—å£èšåˆ

---

## ğŸ“š ä¸‹ä¸€æ­¥

- **æ‰¹é‡ä»»åŠ¡**: [å¹¶è¡Œå¤„ç†](./21_batch_processing.md)
- **å¾®æœåŠ¡ç¼–æ’**: [æœåŠ¡åè°ƒ](./22_microservices.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0.0  
**æœ€åæ›´æ–°**: 2025-10-26  
**ç»´æŠ¤è€…**: temporal-rust æ–‡æ¡£å›¢é˜Ÿ
